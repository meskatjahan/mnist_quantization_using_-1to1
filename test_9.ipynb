{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180284d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow\n",
    "!pip install -q keras\n",
    "\n",
    "\n",
    "## Imports libs\n",
    "import os\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d2cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fa8b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d85fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876bc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff6d36a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m   tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)),\n\u001b[0;32m      3\u001b[0m   tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      4\u001b[0m   tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m      5\u001b[0m   tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      6\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3602657c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(x_train[:\u001b[38;5;241m10\u001b[39m])\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      2\u001b[0m predictions\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = model(x_train[:10]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e29255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06769237, 0.08063873, 0.06308316, 0.0444681 , 0.14171338,\n",
       "        0.10558186, 0.15701105, 0.06409926, 0.04961228, 0.22609982]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cbebe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe218bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2482686"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5238ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e64a5cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 987us/step - loss: 0.3017 - accuracy: 0.9125\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1492 - accuracy: 0.9557\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 922us/step - loss: 0.1133 - accuracy: 0.9647\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 906us/step - loss: 0.0919 - accuracy: 0.9715\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 895us/step - loss: 0.0795 - accuracy: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ed6aeb910>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0847bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0768 - accuracy: 0.9752 - 334ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07683579623699188, 0.9751999974250793]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25cc626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLay  (None, 28, 28)           3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_flatten (QuantizeWrap  (None, 784)              1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrappe  (None, 128)              100485    \n",
      " rV2)                                                            \n",
      "                                                                 \n",
      " quant_dropout (QuantizeWrap  (None, 128)              1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWrap  (None, 10)               1295      \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,785\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 15\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model=model\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fff08bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    # Configure how to quantize weights.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    # Configure how to quantize activations.\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ebe48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "class FixedRangeQuantizer(tfmot.quantization.keras.quantizers.Quantizer):\n",
    "  \"\"\"Quantizer which forces outputs to be between -1 and 1.\"\"\"\n",
    "\n",
    "  def build(self, tensor_shape, name, layer):\n",
    "    # Not needed. No new TensorFlow variables needed.\n",
    "    return {}\n",
    "\n",
    "  def __call__(self, inputs, training, weights, **kwargs):\n",
    "    return tf.keras.backend.clip(inputs, -1.0, 1.0)\n",
    "\n",
    "  def get_config(self):\n",
    "    # Not needed. No __init__ parameters to serialize.\n",
    "    return {}\n",
    "\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n",
    "    # Configure weights to quantize with 4-bit instead of 8-bits.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      # Use custom algorithm defined in `FixedRangeQuantizer` instead of default Quantizer.\n",
    "      return [(layer.kernel, FixedRangeQuantizer())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "518685e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_8 (QuantizeL  (None, 28, 28)           3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_reshape_5 (QuantizeWr  (None, 28, 28, 1)        1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_7 (QuantizeWra  (None, 26, 26, 12)       147       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_max_pooling2d_7 (Quan  (None, 13, 13, 12)       1         \n",
      " tizeWrapperV2)                                                  \n",
      "                                                                 \n",
      " quant_flatten_16 (QuantizeW  (None, 2028)             1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_17 (QuantizeWra  (None, 129)              261744    \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,897\n",
      "Trainable params: 261,861\n",
      "Non-trainable params: 36\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   \n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    \n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Pass in modified `QuantizeConfig` to modify this `Dense` layer.\n",
    "   quantize_annotate_layer(tf.keras.layers.Dense(129,), ModifiedDenseQuantizeConfig()),\n",
    "    \n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2fc02cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_8 (QuantizeL  (None, 28, 28)           3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_reshape_5 (QuantizeWr  (None, 28, 28, 1)        1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_7 (QuantizeWra  (None, 26, 26, 12)       147       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_max_pooling2d_7 (Quan  (None, 13, 13, 12)       1         \n",
      " tizeWrapperV2)                                                  \n",
      "                                                                 \n",
      " quant_flatten_16 (QuantizeW  (None, 2028)             1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_17 (QuantizeWra  (None, 129)              261744    \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,897\n",
      "Trainable params: 261,861\n",
      "Non-trainable params: 36\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# `quantize_model` requires a recompile.\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "270a75b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 6s 11ms/step - loss: 0.5484 - accuracy: 0.8635\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.1874 - accuracy: 0.9474\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.1254 - accuracy: 0.9653\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0978 - accuracy: 0.9727\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0828 - accuracy: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27e9f31e6d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.fit(x_train, y_train,  batch_size=, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa54b879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07915325462818146, 0.9747999906539917]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.evaluate(x_test,  y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "437230a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa152315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9757999777793884\n",
      "Quantize test accuracy: 0.9585999846458435\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = base_model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "_, quant_aware_model_accuracy = quant_aware_model.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quantize test accuracy:', quant_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44537dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as reshape_5_layer_call_fn, reshape_5_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Windows\\TEMP\\tmpg36y0kn6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Windows\\TEMP\\tmpg36y0kn6\\assets\n",
      "C:\\Users\\Meskat Jahan\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model9 = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fde3a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f8eec64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.9579\n",
      "Quant TF test accuracy: 0.9585999846458435\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model9)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', quant_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c6326273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.9579\n",
      "Quant TF test accuracy: 0.9585999846458435\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model9)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', quant_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8012279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Windows\\TEMP\\tmp6v1u8vyy\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Windows\\TEMP\\tmp6v1u8vyy\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 0.389923095703125\n",
      "Quantized model in Mb: 0.25444793701171875\n"
     ]
    }
   ],
   "source": [
    "# Create float TFLite model.\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
    "float_tflite_model10 = float_converter.convert()\n",
    "\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(quantized_tflite_model9)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model10)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735a007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
